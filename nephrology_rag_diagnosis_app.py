import streamlit as st
from PIL import Image, ImageOps
import pytesseract
from pdf2image import convert_from_bytes
import io
import re
import json
import csv
import base64
from io import BytesIO
from docx import Document
from sentence_transformers import SentenceTransformer
import faiss

# Tesseract ê²½ë¡œ ì§€ì • (Windowsìš©)
pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"

st.set_page_config(page_title="ì‹ ì¥ë‚´ê³¼ ì§„ë‹¨ ì‹œìŠ¤í…œ", layout="centered")
st.title("ğŸ” ì‹ ì¥ë‚´ê³¼ ì§ˆë³‘ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (4ë‹¨ê³„: ì§ˆì˜ì‘ë‹µ)")

st.subheader("ğŸ§  ì§ˆë³‘ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”")
user_question = st.text_input("ì˜ˆ: 'eGFRì´ 55ì¸ë° CKDì¸ê°€ìš”?' ë˜ëŠ” 'ë‹¨ë°±ë‡¨ ìˆ˜ì¹˜ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?'")

if user_question:
    st.info("ğŸ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    # ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ
    model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')
    question_embedding = model.encode([user_question])
    index = faiss.read_index("nephro_faiss.index")

    with open("nephro_chunks.json", "r", encoding="utf-8") as f:
        chunks = json.load(f)

    # ê²€ìƒ‰ ìˆ˜í–‰
    D, I = index.search(question_embedding, k=2)

    matched_docs = []
    for idx, score in zip(I[0], D[0]):
        matched_docs.append({
            "title": f"ê´€ë ¨ ë¬¸ì„œ #{idx+1}",
            "similarity": round(1.0 / (1.0 + score), 2),  # L2 ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ê·¼ì‚¬ì¹˜
            "snippet": chunks[idx][:200] + "..." if len(chunks[idx]) > 200 else chunks[idx]
        })

    st.markdown("### ğŸ”— ê²€ìƒ‰ëœ ë¬¸ì„œ ë° ìœ ì‚¬ë„")
    for doc in matched_docs:
        st.write(f"ğŸ“„ **{doc['title']}** â€” ìœ ì‚¬ë„: {int(doc['similarity'] * 100)}%")
        st.caption(f"â¡ï¸ {doc['snippet']}")

    # ëª¨ì˜ LLM ì‘ë‹µ ì˜ˆì‹œ
    llm_answer = "(ì˜ˆì‹œ ì‘ë‹µ) ì…ë ¥í•˜ì‹  ê²€ì‚¬ ìˆ˜ì¹˜ëŠ” CKDì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ì˜ë£Œì§„ì˜ í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤."

    st.markdown("### ğŸ¤– AI ì‘ë‹µ ì˜ˆì‹œ")
    st.success(llm_answer)

    # ë³´ê³ ì„œ ì €ì¥
    if st.button("ğŸ“„ ì§ˆì˜ì‘ë‹µ ë³´ê³ ì„œ ì €ì¥ (DOCX)"):
        doc = Document()
        doc.add_heading("RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ê²°ê³¼ ë¦¬í¬íŠ¸", level=1)
        doc.add_paragraph(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")

        doc.add_heading("ê²€ìƒ‰ëœ ë¬¸ì„œ", level=2)
        for doc_data in matched_docs:
            doc.add_paragraph(f"- {doc_data['title']} (ìœ ì‚¬ë„: {int(doc_data['similarity']*100)}%)")
            doc.add_paragraph(f"  â¤· {doc_data['snippet']}")

        doc.add_heading("AI ì‘ë‹µ", level=2)
        doc.add_paragraph(llm_answer)

        buffer = BytesIO()
        doc.save(buffer)
        st.download_button("DOCX ë‹¤ìš´ë¡œë“œ", data=buffer.getvalue(), file_name="ì§ˆì˜ì‘ë‹µ_ë¦¬í¬íŠ¸.docx")

    st.markdown("---")
    st.markdown("ğŸ“Œ í˜„ì¬ëŠ” ë¡œì»¬ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ì´ë©°, í–¥í›„ LLM ì—°ë™ ì‹œ ì‘ë‹µ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.")

st.divider()
st.markdown("""
ğŸ”„ ê³„ì†í•´ì„œ OCR, ìˆ˜ì¹˜ ì…ë ¥, ì§„ë‹¨ ê²°ê³¼, ë¦¬í¬íŠ¸ ì €ì¥ ë“± ê¸°ì¡´ ê¸°ëŠ¥ì„ í•¨ê»˜ í™œìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
